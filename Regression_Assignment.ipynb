{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Simple Linear Regression?\n",
        "\n",
        "ANS:- Simple Linear Regression is a statistical technique used to model\n",
        "      the relationship between two variables by fitting a straight line to the data. It aims to predict the value of one variable (the dependent variable) based on the value of another variable (the independent variable)."
      ],
      "metadata": {
        "id": "4zZ66FvVP8s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "ANS:- Simple Linear Regression is based on several key assumptions that\n",
        "      ensure the model's validity and the accuracy of its predictions. These assumptions must be met for the results to be reliable. Here are the key assumptions:\n",
        "\n",
        "      1. Linearity:The relationship between the independent variable x\n",
        "                   and the dependent variable ùë¶ must be linear. This means that the change in y should be proportional to the change in x, and the data points should form a straight line when plotted.\n",
        "\n",
        "      2. Independence:The residuals (the differences between the\n",
        "                      observed and predicted values of y) should be independent of each other. This means that the error for one data point should not be correlated with the error for another data point.\n",
        "\n",
        "                      This assumption is particularly important when the data points are collected over time or in a sequence, such as in time series data.\n",
        "\n",
        "      3. Homoscedasticity:The residuals should have constant variance\n",
        "                          across all levels of the independent variablex. This means that the spread of the residuals should be roughly the same for both low and high values of x.\n",
        "\n",
        "                          If the residuals spread out more at one end than the other, this indicates heteroscedasticity, which can lead to biased estimates.\n",
        "\n",
        "      4. Normality of Residuals:The residuals (the differences between\n",
        "                                the observed and predicted values of y) should be approximately normally distributed. This is especially important when you want to conduct hypothesis tests or construct confidence intervals for the regression coefficients.\n",
        "\n",
        "                                If the residuals are not normally distributed, the results of statistical tests might not be valid.\n",
        "\n"
      ],
      "metadata": {
        "id": "3YPpQP5zQo2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "ANS:- In the equation Y=mX+c, the coefficient ùëö represents the slope  \n",
        "      of  the regression line. Specifically, m indicates how much the dependent variable Y changes for a one-unit change in the independent variable X.\n",
        "\n",
        "      Interpretation of m (Slope):\n",
        "        If ùëö is positive, it means that as X increases, Y also increases. This indicates a positive relationship between X and Y.\n",
        "\n",
        "       If ùëö is negative, it means that as X increases, Y decreases. This indicates a negative relationship between X anY.\n",
        "\n",
        "       If m=0, it means there is no relationship between X and Y, and the line is horizontal.\n",
        "\n",
        "      Example:\n",
        "       Imagine you have the equation Y=2X+5. Here, the slope ùëö=2m=2, meaning that for every 1 unit increase in X, Y will increase by 2 units."
      ],
      "metadata": {
        "id": "9OU9FLh8RrdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "ANS:-In the equation Y=mX+c, the intercept c is the value of Y when the\n",
        "     independent variable X is equal to zero.\n",
        "\n",
        "     Interpretation of c (Intercept):The intercept c represents the starting point of the regression line on the Y-axis. It is where the line crosses the Y-axis when X=0.\n",
        "\n",
        "     In other words,c tells you the value of the dependent variable\n",
        "     Y when there is no effect from the independent variable X (because X=0).\n",
        "\n",
        "     Example:Consider the equation =2+5Y=2X+5. Here, the intercept c=5. This means that when X=0, Y will be 5.\n",
        "\n"
      ],
      "metadata": {
        "id": "8-owV2WLTQl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "ANS:-In Simple Linear Regression, the slope ùëö represents the change in\n",
        "     the dependent variable Y for a one-unit change in the independent variable X. The formula to calculate the slope ùëö is derived from minimizing the sum of squared differences (errors) between the observed values of Y and the predicted values of Y.\n",
        "\n",
        "     Formula for the Slope ùëö:\n",
        "     The formula for calculating the slope ùëö is:\n",
        "\n",
        "     m = [n * Œ£(Xi * Yi) - Œ£(Xi) * Œ£(Yi)] / [n * Œ£(Xi^2) - (Œ£(Xi))^2]\n",
        "\n",
        "     Where:\n",
        "      n is the number of data points.\n",
        "\n",
        "      Xi is the value of the independent variable for the i-th data point.\n",
        "\n",
        "      Yi is the value of the dependent variable for the i-th data point.\n",
        "\n",
        "      ‚àë=1‚àëi=1nX i is the sum of all X values.‚àë=1‚àë i=1nY iis the sum of all Y values."
      ],
      "metadata": {
        "id": "TkzUwp6wUVn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.What is the purpose of the least squares method in Simple Linear\n",
        "   Regression?\n",
        "\n",
        "ANS:-The least squares method in Simple Linear Regression is used to\n",
        "     find the best-fitting line (the regression line) that minimizes the difference between the observed data points and the predicted values from the model.\n",
        "\n",
        "    Purpose of the Least Squares Method:\n",
        "     The least squares method in Simple Linear Regression is used to\n",
        "     find the best-fitting line through the data points by minimizing the sum of the squared differences between the observed values and the predicted values of the dependent variable.  It essentially aims to find the line that best represents the relationship between the independent and dependent variables, minimizing the overall error in the prediction."
      ],
      "metadata": {
        "id": "o1NqU5VHWR-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "\n",
        "ANS:-The coefficient of determination (R¬≤) in simple linear regression\n",
        "     represents the proportion of the variance in the dependent variable that is predictable from the independent variable.  It ranges from 0 to 1, where:\n",
        "\n",
        "     R¬≤ = 0 indicates that the independent variable does not explain any of the variability in the dependent variable. The regression line does not fit the data well.\n",
        "\n",
        "     R¬≤ = 1 indicates that the independent variable perfectly explains the variability in the dependent variable.  All data points fall exactly on the regression line.\n",
        "\n",
        "     A value of R¬≤ between 0 and 1 indicates the proportion of variance in the dependent variable explained by the independent variable. For example, an R¬≤ of 0.75 means that 75% of the variance in the dependent variable is explained by the independent variable.\n",
        "\n",
        "     In essence, R¬≤ measures the goodness-of-fit of the regression model.  A higher R¬≤ generally suggests a better fit, but it's crucial to consider other factors like the context of the data and the assumptions of linear regression.  It's not always the case that a higher R¬≤ indicates a better model, as sometimes a simpler model with a slightly lower R¬≤ may be more appropriate and easier to interpret.\n"
      ],
      "metadata": {
        "id": "n1Bqn_OIXRZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.What is Multiple Linear Regression?\n",
        "\n",
        "ANS:-Multiple Linear Regression (MLR) is an extension of Simple Linear\n",
        "     Regression that models the relationship between a dependent variable and two or more independent variables. While Simple Linear Regression involves only one predictor, Multiple Linear Regression allows for the inclusion of multiple predictors (independent variables) to explain the variance in the dependent variable more comprehensively."
      ],
      "metadata": {
        "id": "jnd4AZihYV-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "ANS:-The main difference between Simple Linear Regression and Multiple\n",
        "     Linear Regression lies in the number of independent variables used to predict the dependent variable.\n",
        "\n",
        "     Simple Linear Regression uses only one independent variable to predict the dependent variable, while Multiple Linear Regression uses two or more independent variables."
      ],
      "metadata": {
        "id": "fzLzeLylYmDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "ANS:-The key assumptions of Multiple Linear Regression are:\n",
        "\n",
        "     1. Linearity: The relationship between the dependent\n",
        "        variable and each independent variable is linear.  This means that a change in an  independent variable results in a proportional change in the dependent variable, holding other variables constant.\n",
        "\n",
        "     2. Independence of Errors: The errors (residuals) are\n",
        "        independent of each other.The error for one\n",
        "        observation should not be related to the error for any other observation.  This is particularly important in time  series data where errors might be autocorrelated.\n",
        "\n",
        "     3. Homoscedasticity: The variance of the errors is\n",
        "        constant across all levels  of the independent variables.  This means the spread of the residuals  should be roughly the same for all predicted values. Heteroscedasticity,  where the variance of errors changes, violates this assumption.\n",
        "\n",
        "     4. Normality of Errors: The errors are normally\n",
        "        distributed.  This is important for hypothesis testing and confidence interval calculations.\n",
        "\n",
        "     5. No Multicollinearity: There is no perfect or     near-perfect linear relationship between the independent\n",
        "       variables.  High multicollinearity can make it difficult to determine the individual effects of the  independent variables on the dependent variable."
      ],
      "metadata": {
        "id": "4mVAOM1oZRwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.- What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "ANS:-Heteroscedasticity refers to a situation in regression\n",
        "     analysis where the variance of the errors (residuals) is not constant across all levels of the independent variables. In other words, as the values of the independent variables change, the spread or variability of the residuals also changes.\n",
        "\n",
        "     How does heteroscedasticity affect the results of a Multiple Linear Regression (MLR) model?\n",
        "     1.Inefficiency of estimates: While heteroscedasticity\n",
        "       does not bias the estimated coefficients of the regression (the slopes and intercept), it affects the efficiency of the estimates. The Ordinary Least Squares (OLS) estimator is still unbiased, but it no longer has the minimum variance property. This means that the estimates of the coefficients might be less precise and less reliable.\n",
        "\n",
        "     2.Incorrect standard errors: One of the main\n",
        "       consequences of heteroscedasticity is that it leads to incorrect estimation of the standard errors of the regression coefficients. This can cause misleading results in hypothesis testing, especially for t-tests and F-tests. The standard errors may be underestimated or overestimated, leading to incorrect conclusions about statistical significance.\n",
        "\n",
        "      If standard errors are underestimated, it may lead to Type I errors (false positives) where you incorrectly reject a null hypothesis.\n",
        "\n",
        "      If standard errors are overestimated, it may lead to Type II errors (false negatives) where you fail to reject a false null hypothesis.\n",
        "     3.Ineffective confidence intervals: Heteroscedasticity\n",
        "        also affects the accuracy of confidence intervals for the regression coefficients. Since the standard errors are not reliable, the confidence intervals calculated around the coefficients may not be accurate, and the true values of the coefficients may fall outside of the intervals more often than expected.\n",
        "\n",
        "     4.Model assumptions: Multiple Linear Regression assumes\n",
        "         that the residuals are homoscedastic (i.e., having constant variance). If this assumption is violated, the validity of the regression results is compromised. In such cases, statistical inference (like testing the significance of coefficients) becomes less trustworthy.\n"
      ],
      "metadata": {
        "id": "RIAslHBCTgZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "ANS:-Multicollinearity occurs when two or more independent\n",
        "     variables in a Multiple Linear Regression (MLR) model are highly correlated with each other. This can cause problems in estimating the regression coefficients, making them unstable and difficult to interpret. High multicollinearity inflates the standard errors of the regression coefficients, leading to less reliable hypothesis tests and potentially incorrect conclusions.\n",
        "\n",
        "     Here are several strategies to improve a Multiple Linear Regression model with high multicollinearity:\n",
        "\n",
        "  1. Remove Highly Correlated Variables\n",
        "     Correlation analysis: The first step is to check the correlation between the independent variables. If two or more variables are highly correlated (e.g., correlation coefficient above 0.8 or 0.9), consider removing one of them.\n",
        "     Domain knowledge: Choose the variable that makes the most sense in the context of the problem you're studying, or the one with better data quality.\n",
        "\n",
        "     Variance Inflation Factor (VIF): VIF is a commonly used measure to detect multicollinearity. A high VIF (typically above 5 or 10) indicates that a variable is highly collinear with others. If VIF is high for a particular variable, it can be removed or combined with another variable.\n",
        "\n",
        "    2. Combine Variables (Feature Engineering)\n",
        "       Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that combines correlated variables into a smaller number of uncorrelated components. These components capture most of the variance in the data, and they can be used as predictors in the regression model.\n",
        "\n",
        "       Summing/averaging correlated variables: If the correlated variables are conceptually similar, you can combine them into a single variable, such as by taking the average or sum, to reduce multicollinearity.\n",
        "\n",
        "    3. Use Regularization Techniques (Ridge or Lasso Regression)\n",
        "     Ridge Regression (L2 regularization): This technique adds a penalty term (the sum of squared coefficients) to the loss function. Ridge regression helps shrink the coefficients of correlated variables towards zero, which reduces multicollinearity. It works well when there are many predictors.\n",
        "\n",
        "     Lasso Regression (L1 regularization): Lasso also adds a penalty term, but it can shrink some coefficients to exactly zero, effectively performing variable selection. Lasso is useful if you suspect that only a subset of variables are important.\n",
        "\n",
        "     Elastic Net: A combination of Ridge and Lasso that performs well when there is high multicollinearity and when the number of predictors is large relative to the number of observations.\n",
        "\n"
      ],
      "metadata": {
        "id": "6r2lHs_9UmRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "ANS:-Transforming categorical variables for use in regression\n",
        "     models is a crucial step to ensure that the model can process and interpret these variables effectively. Here are some common techniques used to transform categorical variables:\n",
        "     1.One-Hot Encoding:\n",
        "       This technique creates a new binary (0 or 1) variable for each category in the original categorical variable. For example, if a categorical variable \"Color\" has three categories (Red, Blue, Green), one-hot encoding would create three new variables (Color_Red, Color_Blue, Color_Green), where the value is 1 if the observation belongs to that category and 0 otherwise.\n",
        "     2.Label Encoding:\n",
        "       Label encoding assigns a unique integer to each category. For example, if a variable \"Size\" has categories (Small, Medium, Large), label encoding would assign values like (0, 1, 2). This technique is useful when the categorical variable has an ordinal relationship (i.e., the categories have a natural order).\n",
        "     3.Ordinal Encoding:\n",
        "       This is similar to label encoding but is used specifically for ordinal variables, where the categories have a meaningful order. For example, \"Low,\" \"Medium,\" and \"High\" might be encoded as 0, 1, and 2, respectively.\n",
        "     4.Frequency or Count Encoding:\n",
        "       This technique replaces each category with the frequency or count of its occurrences in the data. For example, if the category \"A\" appears 50 times, \"B\" appears 30 times, and \"C\" appears 20 times, the variable would be encoded as 50, 30, and 20, respectively."
      ],
      "metadata": {
        "id": "e2VoREvZVYDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "ANS:-1. Capturing Combined Effects:\n",
        "        Main effects represent the individual influence of each predictor on the dependent variable.\n",
        "        Interaction effects arise when the relationship between one predictor and the dependent variable depends on the value of another predictor.  The combined effect is not merely the sum of the individual effects.\n",
        "     2. Example:\n",
        "        Suppose we're modeling sales based on advertising spend (TV ads and social media ads).  An interaction term between TV ads and social media ads could capture the synergistic effect of using both advertising channels together.  The combined impact of both types of ads might be greater than simply adding the individual effects.\n",
        "    3. Interpretation:\n",
        "       Interaction terms are typically formed by multiplying the two or more predictors involved.\n",
        "       The coefficient of the interaction term indicates how the relationship between one predictor and the dependent variable changes based on the level of the other interacting predictor.\n",
        "    4. Importance:\n",
        "       Interaction terms can significantly improve the accuracy of the regression model if the relationship between predictors and the dependent variable is non-additive.\n",
        "       Ignoring interaction effects might lead to an incomplete understanding of the underlying relationship between variables.\n",
        "       Including interaction terms can reveal complex relationships that simple main effects cannot capture.\n"
      ],
      "metadata": {
        "id": "U7ISBf6MbLAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.- How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "ANS:In Simple Linear Regression:\n",
        "      The intercept represents the predicted value of the dependent variable when the independent variable is zero.  It's the point where the regression line crosses the y-axis.  It's often meaningful and interpretable in this context.\n",
        "\n",
        "  In Multiple Linear Regression:\n",
        "     The intercept's interpretation becomes more nuanced. It represents the predicted value of the dependent variable when *all* independent variables are zero.  This is often not a realistic or meaningful scenario.  For example, if you're predicting house prices based on square footage and the number of bedrooms, an intercept where both square footage and number of bedrooms are zero doesn't correspond to a real-world situation. The interpretation is primarily mathematical."
      ],
      "metadata": {
        "id": "cccph0jdcsCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "ANS:- The slope (m) in regression analysis represents the\n",
        "      change in the dependent variable (Y) for a one-unit change in the independent variable (X), holding all other variables constant (in multiple linear regression).  It quantifies the strength and direction of the linear relationship between the variables.\n",
        "\n",
        "      Significance and Impact on Predictions:\n",
        "       1. Direction of Relationship: A positive slope\n",
        "            indicates a positive relationship, meaning as X increases, Y tends to increase.  A negative slope indicates a negative relationship, meaning as X increases, Y tends to decrease.  The magnitude of the slope reflects the steepness of the relationship.\n",
        "      2. Prediction: The slope directly affects the predicted\n",
        "            values of Y. For each one-unit increase in X, the predicted value of Y changes by the amount specified by the slope.  This is the core of how regression makes predictions.  The slope is used to calculate the predicted Y for a given X value.\n",
        "     3. Interpretation in Context: The interpretation of the\n",
        "           slope depends heavily on the units of measurement for X and Y.  The coefficient's magnitude is only meaningful when viewed in relation to these units.\n",
        "     4. Statistical Significance: Hypothesis testing\n",
        "           determines if the slope is significantly different from zero. A statistically significant non-zero slope implies that there is a real relationship between X and Y and that the model can make useful predictions.\n",
        "     5. Effect Size: The size of the slope (its magnitude)\n",
        "           indicates the effect size; how much Y changes given a unit change in X.  A larger magnitude suggests a stronger effect."
      ],
      "metadata": {
        "id": "AZ9HcDnHdOTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "ANS:-1. General Definition of the Intercept:\n",
        "       In a regression model, the intercept is the value of the dependent variable\n",
        "       Y when all independent variables 1,2,‚Ä¶,X 1,X 2,‚Ä¶,Xk are equal to zero. Mathematically:\n",
        "\n",
        "     2. Point of Reference:  The intercept provides a      reference point for interpreting the slope.  The slope indicates how much the dependent variable changes for a one-unit change in the independent variable. The intercept indicates where this change begins.\n",
        "\n",
        "     3. Understanding the Model's Predictions:  The intercept\n",
        "         is a component of the prediction equation.  Together with the slope(s) and predictor variables, the intercept determines the predicted value of the dependent variable for any given set of inputs.\n",
        "\n",
        "    4. Contextual Interpretation: The practical meaning of\n",
        "       the intercept depends heavily on the context of the problem.  Sometimes, the intercept has a clear and meaningful real-world interpretation. Other times, its interpretation might not be directly relevant or might be extrapolated to a nonsensical value (e.g., a house price model where all features are 0)."
      ],
      "metadata": {
        "id": "JFnQ7S1Ed6z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "ANS:What are the limitations of using R¬≤ as a sole measure of\n",
        "    model performance\n",
        "\n",
        "    1. Doesn't account for model complexity: R¬≤ always\n",
        "       increases with the addition of more predictors to the model, even if those predictors do not significantly improve the model's predictive power.  This can lead to overfitting, where a model fits the training data very well but performs poorly on new, unseen data.  A model with more predictors and a higher R¬≤ may not be better than a simpler model with a slightly lower R¬≤.  Adjusted R¬≤ tries to address this issue by penalizing the addition of unnecessary predictors.\n",
        "\n",
        "    2. Doesn't indicate prediction accuracy: A high R¬≤ does\n",
        "       not guarantee that the model makes accurate predictions.  It only measures how well the model fits the data, but the data itself might be noisy or contain errors, so even a well-fitting model might not be very predictive.\n",
        "\n",
        "    3. Assumes linearity: R¬≤ assumes a linear relationship\n",
        "       between the variables. If the relationship is non-linear, R¬≤ might not be a good measure of fit, and other metrics would be more appropriate.\n",
        "\n",
        "    4. Sensitive to outliers: Outliers can heavily influence\n",
        "       R¬≤, inflating or deflating it depending on their location and magnitude.  A model with a high R¬≤ might be highly influenced by a small number of extreme data points.\n",
        "\n",
        "    5. Not suitable for all types of models: R¬≤ is primarily\n",
        "       designed for linear regression models.  Its interpretation in other types of models, such as classification models, is not always straightforward or meaningful."
      ],
      "metadata": {
        "id": "oJaKPKXVfJiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19. How would you interpret a large standard error for a regression coefficient?\n",
        "ANS:-A large standard error for a regression coefficient\n",
        "     indicates a high degree of uncertainty in the estimated value of that coefficient.  It means that the coefficient's true value could be quite different from the estimated value.  Several factors can cause this.  The most common include:\n",
        "\n",
        "     **High variability in the data:** If the data points are   very spread out around the regression line, the standard error will be larger, making it more difficult to pinpoint the precise effect of the independent variable on the dependent variable.\n",
        "     **Small sample size:** A smaller sample provides less\n",
        "        information, leading to more uncertainty in the coefficient estimates.  Consequently, the standard errors tend to be larger.\n",
        "     **Multicollinearity:** When predictor variables in a\n",
        "       multiple regression are highly correlated, it becomes difficult to isolate the effect of each one. This inflated correlation increases the standard errors of the coefficients.\n",
        "     **Inappropriate model specification:** Using the wrong\n",
        "       type of model (linear when a non-linear model is appropriate, for example) or ignoring relevant variables can inflate the standard errors.\n",
        "     **Outliers:** Extreme values in the data can heavily\n",
        "       influence the regression coefficients and increase their standard errors."
      ],
      "metadata": {
        "id": "1UyfXw5ifp7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "ANS: Heteroscedasticity refers to a condition in regression\n",
        "     analysis where the variance of the residuals (errors) is not constant across all levels of the independent variable(s). In other words, the spread (or variability) of the errors changes as the value of the predictor variable(s) changes. This violates one of the key assumptions of linear regression, which assumes homoscedasticity (constant variance of errors).\n",
        "\n",
        "     o detect heteroscedasticity, you can create a residual vs. fitted values plot (or a residual vs. predictor variable plot if you suspect heteroscedasticity in relation to a specific predictor). Here‚Äôs what you should look for in this plot:\n",
        "\n",
        "     Homoscedasticity (No Heteroscedasticity): When the variance of the residuals is constant, the residuals will appear to be randomly scattered around zero, with no obvious pattern. The spread of residuals will be roughly the same across all values of the fitted values (or predictor).\n",
        "\n",
        "     Appearance: The residuals will form a cloud of points scattered randomly around the horizontal line y=0 without any discernible structure.\n",
        "\n",
        "     Why is it Important to Address Heteroscedasticity?\n",
        "       Heteroscedasticity is important to address in regression models because it can have several detrimental effects:\n",
        "\n",
        "    1. Inefficient Estimates:\n",
        "       In the presence of heteroscedasticity, the ordinary least squares (OLS) estimators of the regression coefficients remain unbiased, but they are no longer efficient (i.e., they do not have the smallest possible variance among all unbiased estimators). This means that the standard errors of the estimated coefficients may be incorrect, leading to unreliable hypothesis tests and confidence intervals.\n",
        "\n",
        "       Example: A large standard error might result in the coefficient appearing not statistically significant, even though it could be significant if the correct variance of residuals were accounted for.\n",
        "\n",
        "       2. Incorrect Inferences:\n",
        "          Since the standard errors of the coefficients may be biased due to heteroscedasticity, tests like the t-test for individual coefficients and the F-test for the overall model may be invalid. This increases the risk of Type I (false positive) or Type II (false negative) errors in hypothesis testing.\n",
        "\n",
        "         The p-values computed from the standard errors may not be valid, leading to incorrect conclusions about the relationships between the predictors and the outcome.\n",
        "\n"
      ],
      "metadata": {
        "id": "6sItp88-gMco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "\n",
        "ANS: A high R¬≤ but a low adjusted R¬≤ indicates that your\n",
        "     model may be overfitting the training data.R¬≤ measures the proportion of variance in the dependent variable explained by the model. A high R¬≤ suggests a good fit to the data.  However, R¬≤ can be artificially inflated by adding more predictors to the model, even if those predictors don't genuinely improve predictive power.\n",
        "\n",
        "     The adjusted R¬≤, on the other hand, penalizes the addition of unnecessary predictors.A low adjusted R¬≤ despite a high R¬≤ suggests that the model's performance is likely not generalizing well to new data.It's likely the inclusion of numerous predictors, some of which may be irrelevant or noisy, has improved the model's fit to the training data but at the cost of reduced predictive ability for unseen data.\n"
      ],
      "metadata": {
        "id": "NzNkAtsylRUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22.Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "ANS:-1. Feature Scaling and Gradient Descent:\n",
        "        Many optimization algorithms used to estimate coefficients in linear regression, such as gradient descent, are sensitive to the scale of the input features. If features have vastly different scales, features with larger values can disproportionately influence the optimization process. This can lead to slow convergence, instability, and potentially suboptimal solutions. Scaling ensures that all features contribute equally to the optimization process.\n",
        "\n",
        "     2. Regularization Techniques:\n",
        "        Regularization methods, like Ridge and Lasso regression, add penalty terms to the loss function to prevent overfitting. These penalties are sensitive to the scale of the features. Without scaling, features with larger values will be penalized more heavily than those with smaller values, even if they're equally important. Scaling ensures that the regularization terms affect all features fairly.\n",
        "\n",
        "     3. Distance-Based Algorithms (if applicable):\n",
        "        While not directly applicable to standard linear regression, some related techniques like k-nearest neighbors (k-NN) or support vector machines (SVMs) use distance metrics.  In such cases, features with larger scales can dominate the distance calculations.  Scaling ensures that all features contribute equally to the distance metric.\n",
        "\n",
        "     4. Improved Interpretability (sometimes):\n",
        "        While scaling does not change the R-squared or other goodness-of-fit measures, it can make the coefficients more directly comparable.  When features are on similar scales, the magnitude of the coefficients gives a better indication of the relative importance of each feature in predicting the target variable.\n",
        "\n",
        "     5. Numerical Stability:\n",
        "        Scaling can improve the numerical stability of the calculations involved in estimating regression coefficients. Features with extremely large or small values can sometimes lead to numerical errors."
      ],
      "metadata": {
        "id": "9tVMeXEil6wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.What is polynomial regression?\n",
        "\n",
        "ANS:-Polynomial regression is a form of regression analysis\n",
        "     in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial.  In simpler terms, it's a way to fit a curve to your data instead of a straight line (as in linear regression)."
      ],
      "metadata": {
        "id": "7DK1jb5XmdQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24.How does polynomial regression differ from linear regression?\n",
        "\n",
        "ANS:- Linear Regression:\n",
        "      Models the relationship between variables as a straight line.Assumes a linear relationship between the independent and dependent variables. Equation: y = mx + c (where m is the slope and c is the intercept)\n",
        "\n",
        "      Polynomial Regression:\n",
        "      Models the relationship between variables as a curve (polynomial function). Can capture non-linear relationships between variables. Equation: y = a + b1x + b2x^2 + ... + bnx^n (where n is the degree of the polynomial)\n",
        "\n",
        "       Key Differences:\n",
        "     1. Relationship Modeling:\n",
        "        Linear regression models a straight-line relationship.Polynomial regression models a curved relationship, allowing for more complex patterns in the data.\n",
        "\n",
        "     2. Flexibility:    \n",
        "        Linear regression is less flexible; it can only capture linear trends.Polynomial regression is more flexible; by increasing the degree of the polynomial, it can fit more complex curves to the data, potentially capturing non-linear patterns more accurately."
      ],
      "metadata": {
        "id": "2LZBsVl-mzK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25.When is polynomial regression used?\n",
        "\n",
        "ANS: Polynomial regression is used when the relationship\n",
        "     between the independent and dependent variables is not linear.  It's used to model curves and capture non-linear patterns in data.  Here are some specific situations:\n",
        "\n",
        "     1. Non-linear relationships: When a scatter plot of your\n",
        "        data reveals a curved pattern, a polynomial regression might be a suitable choice.  A straight line from linear regression wouldn't fit the data well.\n",
        "\n",
        "     2. Modeling complex trends:  When there are turning\n",
        "        points or fluctuations in the data, polynomial regression can model these trends more effectively.\n",
        "\n",
        "     3. Specific applications:\n",
        "        Predicting growth or decay:  Examples include population growth, the spread of diseases, radioactive decay, etc.\n",
        "        Modeling physical phenomena:  Certain physical phenomena are naturally described by polynomial relationships.\n",
        "        Financial modeling:  Sometimes, stock prices or other financial indicators follow curved patterns that are better modeled by polynomial regression.\n",
        "        Image processing:  For example, curve fitting in image analysis and processing.\n"
      ],
      "metadata": {
        "id": "EeJ6twn2nbIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26.What is the general equation for polynomial regression?\n",
        "\n",
        "ANS:- The general equation for polynomial regression is:\n",
        "\n",
        "      y = b0 + b1*x + b2*x^2 + b3*x^3 + ... + bn*x^n\n",
        "\n",
        "      where:\n",
        "       y is the dependent variable\n",
        "       x is the independent variable\n",
        "       b0 is the intercept (the value of y when x = 0)\n",
        "       b1, b2, b3, ..., bn are the coefficients of the polynomial terms\n",
        "       n is the degree of the polynomial (the highest power of x)\n",
        "\n",
        "       Example (quadratic polynomial, n=2):\n",
        "\n",
        "       y = b0 + b1*x + b2*x^2"
      ],
      "metadata": {
        "id": "weWRyw_loDab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q27.Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "ANS:- Yes, polynomial regression can be applied to multiple\n",
        "      variables.  Instead of having terms like x^2 or x^3, you'll have interaction terms and polynomial terms for each of the independent variables.\n",
        "\n",
        "      For example, with two independent variables (x1 and x2), a second-degree  polynomial regression model might look like this:\n",
        "\n",
        "      y = b0 + b1*x1 + b2*x2 + b3*x1^2 + b4*x2^2 + b5*x1*x2\n",
        "\n",
        "     In this equation:\n",
        "     b0 is the intercept.\n",
        "     b1 and b2 represent the linear effect of x1 and x2, respectively.\n",
        "     b3 and b4 represent the quadratic effect of x1 and x2, respectively.\n",
        "     b5 represents the interaction effect between x1 and x2."
      ],
      "metadata": {
        "id": "Y9NrEYoCoyfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q28.What are the limitations of polynomial regression?\n",
        "\n",
        "ANs:- 1. Overfitting:  Polynomial regression models,\n",
        "         especially those with high degrees, can easily overfit the training data. This means that the model fits the training data very well but performs poorly on new, unseen data.\n",
        "      2. Sensitivity to Outliers: Polynomial regression\n",
        "         models are sensitive to outliers in the data, similar to linear regression.  Outliers can disproportionately influence the coefficients, distorting the fitted curve.\n",
        "      3. Extrapolation: Extrapolating beyond the range of the\n",
        "         observed data can be highly unreliable. Polynomial    functions can produce extreme values outside of the     observed data range, and there's no guarantee that the model will accurately represent the true relationship.\n",
        "      4. Interpretability:  As the degree of the polynomial\n",
        "         increases, the model becomes more complex and less\n",
        "         interpretable.  It can be difficult to understand the meaning of the coefficients and how they contribute to the model's predictions."
      ],
      "metadata": {
        "id": "Cj1mTzuDpY88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "ANS:- Several methods can be used to evaluate model fit and\n",
        "      select the degree of a polynomial:\n",
        "      1. **R-squared (R¬≤):** While R¬≤ always increases with\n",
        "         the addition of more polynomial terms (higher degree), it's a good starting point.  Look for a point of diminishing returns, where adding more terms doesn't substantially improve R¬≤. However, don't rely on R¬≤ alone.\n",
        "      2. **Adjusted R-squared (Adjusted R¬≤):** This metric\n",
        "         penalizes the addition of unnecessary predictors (polynomial terms). An adjusted R¬≤ that starts decreasing as you add more terms suggests overfitting.  It's a more reliable indicator than R¬≤ for model selection.\n",
        "      3. **Cross-validation (k-fold CV):** Divide the data\n",
        "         into k folds. Train the polynomial regression model on k-1 folds and test it on the remaining fold. Repeat this process for each fold.  Calculate a performance metric (like Mean Squared Error or Mean Absolute Error) on each fold and average the results.  This gives a more robust estimate of the model's performance on unseen data and helps to identify the degree that minimizes the error on unseen data.\n",
        "     4. **Residual analysis:** Plot residuals vs. fitted\n",
        "        values to check for patterns. A random scatter of residuals around zero indicates a good fit.  Patterns in the residuals (like a funnel shape, for example) suggest a problem like heteroscedasticity or a need for a different degree polynomial."
      ],
      "metadata": {
        "id": "VK_tnAhNqJic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30. Why is visualization important in polynomial regression?\n",
        "\n",
        "ANS:-Visualization is crucial in polynomial regression for\n",
        "     several reasons:\n",
        "\n",
        "     1. Identifying Non-linearity: Plots of the data can\n",
        "        reveal non-linear patterns that suggest the need for polynomial regression in the first place.  A simple scatter plot of the dependent variable against the independent variable can clearly show if a straight line (linear regression) is a poor fit.\n",
        "\n",
        "     2. Degree Selection: Visualizing the fitted polynomial\n",
        "        curves for different degrees helps determine the optimal degree.  Plotting the data and the fitted curves for various degrees allows you to see which polynomial degree best captures the underlying trend without overfitting the data.  Overly complex polynomials (high degrees) can fit the noise in the data, while overly simple polynomials may fail to capture the true relationship.\n",
        "\n",
        "     3. Evaluating Model Fit: Visual comparison of the fitted\n",
        "        curve to the data points provides a quick assessment of how well the model fits the data. You can see whether the curve closely follows the data points or deviates significantly, indicating areas where the model might be inaccurate.\n",
        "\n",
        "    4. Detecting Outliers and Influential Points:       Visualization can help identify outliers or influential points that might be disproportionately affecting the model. Outliers can significantly alter the fitted polynomial, and their presence should be investigated.\n",
        "\n",
        "   5. Understanding Residuals: Residual plots (residuals vs.\n",
        "      fitted values) can reveal patterns that indicate a poor fit or violations of model assumptions.  A randomly scattered pattern of residuals around zero suggests a good fit. Non-random patterns can indicate that a polynomial of a different degree or a different model altogether might be more appropriate."
      ],
      "metadata": {
        "id": "ZmA_2OsQqv4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q31.How is polynomial regression implemented in Python?"
      ],
      "metadata": {
        "id": "S8Jhst2FrKm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Sample data (replace with your own data)\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable\n",
        "y = np.array([2, 4, 5, 4, 5])  # Dependent variable\n",
        "\n",
        "# Choose the degree of the polynomial\n",
        "degree = 2  # Example: Quadratic polynomial\n",
        "\n",
        "# Create polynomial features\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit linear regression model to the polynomial features\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X, y, label='Data')\n",
        "plt.plot(X, y_pred, color='red', label=f'{degree}-degree Polynomial')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Polynomial Regression')\n",
        "plt.show()\n",
        "\n",
        "# Print the coefficients\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "82p7FNovreJ4",
        "outputId": "a67a4387-1dbe-42a2-bed9-60152c9ad7b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [ 0.          2.31428571 -0.28571429]\n",
            "Intercept: 0.20000000000001128\n"
          ]
        }
      ]
    }
  ]
}